{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kanza Nasim\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'text'],\n",
      "        num_rows: 144\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"KANZOO/scrapped_articles\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Access the 'train' subset\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "\n",
    "# Function to merge all text into one large dataset\n",
    "def merge_all_text(dataset):\n",
    "    combined_text = \"\"\n",
    "    for example in dataset['train']:\n",
    "        combined_text += example['text'] + \" \"  \n",
    "    return combined_text.strip()\n",
    "\n",
    "\n",
    "# Merge all text from the dataset\n",
    "combined_text = merge_all_text(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Word Count: 247616\n"
     ]
    }
   ],
   "source": [
    "# Function to count the number of words\n",
    "def count_words(text):\n",
    "    words = text.split()  \n",
    "    return len(words)\n",
    "\n",
    "# Count the words in the merged text\n",
    "word_count = count_words(combined_text)\n",
    "\n",
    "# Print the total word count\n",
    "print(\"Total Word Count:\", word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Function to remove all punctuation except for Urdu period (۔)\n",
    "def remove_punctuation(text):\n",
    "    # Regex pattern to remove all punctuation except for \"۔\"\n",
    "    pattern = r'[^\\w\\s۔]'  \n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# Remove all punctuation except for \"۔\"\n",
    "cleaned_text = remove_punctuation(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Word Count: 247421\n"
     ]
    }
   ],
   "source": [
    "# Function to count the number of words\n",
    "def count_words(cleaned_text):\n",
    "    words = cleaned_text.split()  \n",
    "    return len(words)\n",
    "\n",
    "# Count the words in the merged text\n",
    "word_count = count_words(cleaned_text)\n",
    "\n",
    "# Print the total word count\n",
    "print(\"Total Word Count:\", word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentence Count: 9251\n"
     ]
    }
   ],
   "source": [
    "# Function to count the number of sentences based on the period \"۔\"\n",
    "def count_sentences(cleaned_text):\n",
    "    sentences = cleaned_text.split('۔') \n",
    "    # Filter out empty strings resulting from splitting\n",
    "    return len([s for s in sentences if s.strip()])  \n",
    "\n",
    "# Get the sentences in the cleaned text\n",
    "sentences = count_sentences(cleaned_text) \n",
    "\n",
    "# Print the total sentence count\n",
    "print(\"Total Sentence Count:\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove the Urdu period \"۔\" from the text\n",
    "def remove_period(text):\n",
    "    cleaned_text_without_period = text.replace('۔', '')  \n",
    "    return cleaned_text_without_period\n",
    "\n",
    "cleaned_text_without_period = remove_period(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Similarity Thresholding for SBD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kanza Nasim\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained mBERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = cleaned_text_without_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Tokenize the input text in batches\n",
    "def chunk_text(text, chunk_size=128):\n",
    "    \"\"\"Split text into smaller batches.\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Process each batch through BERT and collect embeddings\n",
    "def process_batches(token_batches):\n",
    "    all_embeddings = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for token_batch in token_batches:\n",
    "        # Convert tokens to token ids\n",
    "        inputs = tokenizer.convert_tokens_to_ids(token_batch)\n",
    "        inputs = torch.tensor([inputs])  \n",
    "        \n",
    "        # Get embeddings from BERT\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            last_hidden_states = outputs.last_hidden_state \n",
    "        \n",
    "        # Collect the embeddings and tokens\n",
    "        embeddings = last_hidden_states.squeeze(0).numpy()  \n",
    "        all_embeddings.append(embeddings)\n",
    "        all_tokens.extend(token_batch)\n",
    "    \n",
    "    # Concatenate embeddings for all batches\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    return all_tokens, all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into batches and process\n",
    "token_batches = chunk_text(input_text)\n",
    "tokens, embeddings = process_batches(token_batches)\n",
    "\n",
    "# Normalize embeddings for comparison\n",
    "norm_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Detect sentence boundaries based on similarity threshold\n",
    "threshold = 0.6 \n",
    "cosine_similarities = np.array([np.dot(norm_embeddings[i], norm_embeddings[i+1]) for i in range(len(norm_embeddings)-1)])\n",
    "\n",
    "detected_sentences = []\n",
    "current_sentence = []\n",
    "\n",
    "for i, token in enumerate(tokens[:-1]):  \n",
    "    current_sentence.append(token)\n",
    "    \n",
    "    # If the cosine similarity drops below the threshold, it indicates a sentence boundary\n",
    "    if cosine_similarities[i] < threshold:\n",
    "        detected_sentences.append(\" \".join(current_sentence))\n",
    "        current_sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Sentence 1: اس\n",
      "Detected Sentence 2: س\n",
      "Detected Sentence 3: ##لس\n",
      "Detected Sentence 4: ##لے\n",
      "Detected Sentence 5: کی\n",
      "Detected Sentence 6: دیگر\n",
      "Detected Sentence 7: ا\n",
      "Detected Sentence 8: ##ق\n",
      "Detected Sentence 9: ##سا\n",
      "Detected Sentence 10: ##ط\n",
      "\n",
      "Total sentences detected: 393397\n"
     ]
    }
   ],
   "source": [
    "# Add the last sentence if not empty\n",
    "if current_sentence:\n",
    "    detected_sentences.append(\" \".join(current_sentence))\n",
    "\n",
    "# Print only the first few detected sentences \n",
    "for i, sentence in enumerate(detected_sentences[:10]):  \n",
    "    print(f\"Detected Sentence {i+1}: {sentence}\")\n",
    "\n",
    "# Optionally, print the total number of detected sentences\n",
    "print(f\"\\nTotal sentences detected: {len(detected_sentences)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
