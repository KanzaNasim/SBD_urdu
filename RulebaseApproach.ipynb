{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kanza Nasim\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"KANZOO/scrapped_articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'text'],\n",
      "        num_rows: 144\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'کراچی: صدیوں کی کتھا! (تیسری قسط)', 'text': 'اس سلسلے کی دیگر اقساط یہاں پڑھیے۔\\n\\nیہ کیسے ممکن ہے کہ کسی کا مُستقبل ماضی کے بغیر ہو؟ کیونکہ دن دن سے اور پل پل سے جُڑا ہوتا ہے۔ جس طرح ہم گزرے دن کو بھی کل کہتے ہیں اور آنے والے دن کو بھی کل کہتے ہیں، اس لیے کیونکہ کل اور کل میں جو آج ہے وہ ایک کڑی ہے جو دونوں کو آپس میں جوڑ کر رکھتی ہے۔ تو ہم اگر کبھی تنہائی میں کچھ پل نکال کر سوچیں کہ ہماری جتنی بھی زندگی گزری خواہ وہ 24 برس ہو، 64 کی یا 84 برس ہو، ہم یہ حیات اسی صورت میں گزار پاتے ہیں جب ہم اپنے گزرے برسوں اور شب و روز سے جُڑے رہتے ہیں۔\\n\\nاگر لوگوں، قوموں اور تہذیبوں سے اُن کا ماضی چھین لیا جائے تو وہ شاید زیادہ عرصہ زندہ نہیں رہ پائیں گی۔ اُن میں آگے بڑھنے کی چاہت ختم ہوجائے گی، تگ و دو کا جوہر ان سے چھن جائے گا بالکل ایسے جیسے کسی وجود کو الزائمر کی بیماری لگتی ہے اور اُس کی یادداشت کے خلیے بے جان ہونے لگتے ہیں۔ آپ یہ سمجھیں کہ یہ بیماری اُس سے اُس کا ماضی چھین لیتی ہے۔ اور وہ اِسی کیفیت میں زیادہ زندگی نہیں جی سکے گا اور اگر جیے گا بھی تو حیات کے اُن رنگوں، ذائقوں اور کیفیتوں کے ریگستان میں جہاں موت کی ویرانی کے سوا کچھ نہیں بچتا۔ نہ رنگوں کے پھول کھلتے ہیں، نہ ذائقوں کی خوشبو کا کوئی جھونکا آتا ہے اور نہ غم و خوشی کی کوئی فاختہ اُڑتی ہے اور نہ آسمان کی نیلاہٹ کے گہرے رنگ میں کشش کی کوئی ناؤ چلتی ہے۔ بس ایک بے رنگ اور بے ذائقہ حیات۔\\n\\nہمارا آج کا سفر بھی سمندر کنارے اور ان پہاڑیوں کے جنگلات، آبشاروں، غاروں اور قدامت کی ان پگڈنڈیوں پر ہی ہے مگر آج ہمیں ڈاکٹر عبدالرؤف کی کراچی ریجن پر کی گئی تحقیق اور اُس تحقیق پر پاؤلو بیگی کی تازہ تحقیق پر تفصیلی بات کرنی تھی، لیکن جیسے ہی ہم سفر کرنے کی تیاری میں تھے تو ہمارے ساتھیوں میں سے کسی نے دو تین سوال کر ڈالے کہ کھیرتھر کے اس پہاڑی سلسلے پر ارتقا کا عمل کتنا قدیم ہوسکتا ہے اور ڈاکٹر عبدالرؤف سے پہلے بھی کراچی پر اس حوالے سے تحقیق ہوئی ہے یا نہیں؟\\n\\nمختلف زمانوں کے نام اور ان کی عمریں—تصویر: ایکس\\n\\nیہ سوالات اپنی جگہ پر اہم ہیں اور ان سوالات کا جواب ڈاکٹر صاحب کی رپورٹ ’اینشیئنٹ سیٹلمنٹس ان کراچی ریجن‘ (Ancient Settlements in Karachi Region) میں ضرور موجود ہوگا۔ مگر میں یہاں ہربرٹ جارج ویلز (Herbert George Wells) کی مشہور تحقیقی کتاب ’دی آؤٹ لائن آف ہسٹری‘ سے دو نقشے آپ کے سامنے پیش کررہا ہوں جو آج سے 25 اور 50 ہزار سال پہلے کے جغرافیائی حالات کو ظاہر کرتے ہیں۔\\n\\n50 ہزار سال پہلے کا نقشہ\\n\\n50 ہزار برس پہلے والے نقشے کو دیکھیں گے تو آپ کو میدانی اور ریگستانی سندھ، پورا پنجاب، گنگا گھاٹی پانی کے نیچے نظر آئیں گے جبکہ سندھ کے مغربی اونچائی والے پہاڑی سلسلے اور بلوچستان آپ کو پانی سے آزاد نظر آئیں گے۔ جبکہ 35 سے 25 ہزار برس پہلے والے نقشے میں آپ کو سندھ کے میدانی اور ریگستانی حصے چولستان اور کَچھ پانی کے نیچے نظر آئیں گے اور باقی بہت سارے علاقوں سے پانی زمین کو چھوڑ چکا ہے۔ ایسے منظرنامے کو ذہن میں رکھ کر مانیک بی پٹھاوالا (Maneck B Pithawala) تحقیق پر مبنی اپنی تحریر میں لکھتے ہیں کہ یہ ماحول انسان کی خوراک اور ترقی کے لیے انتہائی شاندار تھا۔\\n\\n35 سے 25 ہزار برس پہلے والے نقشے\\n\\nڈاکٹر عبدالرؤف صاحب سے پہلے ہیو ٹریور لیمبرک (Hugh Trevor Lambric) نے انتہائی تفصیل سے سندھ کی ارضیات اور اُس کی تاریخ اور تاریخی مقامات پر تحقیق کی۔ نانی گوپال ماجمدر (Nani Gopal Majumdar) نے اس پہاڑی سلسلے کی تاریخ سے پہلے کے مقامات کی کھدائی اور سروے کیا تھا۔ 1976ء میں کیمبرج یونیورسٹی سے پاکستان آرکیالوجی پر کام کرنے کے لیے آرکیالوجسٹ بریجٹ آلچن (Bridget Allchin) اپنے شوہر آلچن کے ہمراہ یہاں آئی اور اس نے تقریباً پورے پاکستان میں تحقیقی کام کیا۔\\n\\nاس حوالے سے اس کی کتاب ’دی رائز آف سیویلائزیشن اِن انڈیا اینڈ پاکستان‘ شائع ہوئی جس میں اُنہوں نے پاکستان کی آرکیالوجی سائٹس کے حوالے سے انتہائی اہم تحقیق کی اور تحقیق کی بنیاد پر کہا کہ یہاں خاص طور پر سندھو گھاٹی کے جنوبی اور مغربی کنارے کے اطراف جن میں ملیر اور لیاری ندی کے بیچ اور اُس سے مغرب حب، لسبیلہ اور شمال بلوچستان میں، آخری پلائسیٹوسین پیلیولیتھک لوگوں کے رہنے کے لیے مناسب ماحول تھا کیونکہ یہاں کے جغرافیائی حالات جو دریائے سندھ کے بہاؤ کی وجہ سے تخلیق ہوئے تھے اُن میں جنوب مغرب میں میسولیتھک اور پیلیولیتھک صنعتیں موجود تھیں۔\\n\\nہولوسینو زمانے میں انسانی بستی\\n\\nدریائے سندھ کے بہاؤ کے مغرب میں سلیمان اور کھیرتھر کے پہاڑی سلسلے ہیں جبکہ اُس کے مشرق میں ریگستان کا ایک وسیع بیلٹ ہے۔ دریا نے ان دونوں کے بیچ میں اپنے بہاؤ کے ساتھ لائی ہوئی مٹی کی ایک زرخیز تہہ جمائی جو دونوں طرف رہنے والوں کے لیے ایک پُرکشش علاقہ بنا ہوگا کیونکہ میٹھے پانی کے ساتھ گھنے جنگلات، جنگلی حیات اور نباتات کی ایک وسیع کائنات اُس میں سمائی ہوئی تھی۔\\n\\nاس محقق جوڑے نے جنوبی بلوچستان میں بالاکوٹ، سندھ میں کراچی، گُجو، اونگر، حیدرآباد، عمرکوٹ، کوٹڈیجی، روہڑی، موہن جو دڑو، آمری، شمالی بلوچستان میں جھڈیر، مہرگڑھ، پیرک (سبی)، پنجاب میں ملتان سے دیرہ جات، گومل، لاہور، ٹیکسلا، پشاور تک تحقیقی کام کیا تھا اس لیے اُس کی رائے کی خاص اہمیت ہے۔\\n\\nاس سفر میں ہم جب سندھو گھاٹی تہذیب اور زرعی بستیوں کی بات کریں گے تو ہمیں اُمید ہے کہ بریجٹ اور آلچن ہماری ضرور مدد کریں گے۔ اب ہم پروفیسر عبدالرؤف خان کی تحقیقی رپورٹ کی بات کرتے ہیں جو جنوبی سندھ کے سمندری کنارے اور برساتی ندیوں، یہاں قدیم انسان اور ماحول کے حوالے سے انتہائی اہم تحقیق ہے۔ چونکہ یہ ایک طویل رپورٹ ہے جس کو مکمل طور پر یہاں بیان کرنا ممکن نہیں ہے تو کوشش کرتے ہیں کہ اس کا لُب لباب یہاں بیان کیا جا سکے۔ اس تحقیق سے یہ سمجھنے میں آسانی رہتی ہے کہ منگھو پیر، مول، ملیر، لیاری، کھدیجی اور کھیرتھر پہاڑی سلسلے پر وہ کیا معروضی حالات تھے جن کی وجہ سے یہاں ابتدائی انسان کے رہنے کے لیے سازگار ماحول بنا۔\\n\\nکھدیجی اور مول کی وادیاں\\n\\nمسلسل ارضیاتی تبدیلیوں جن کا ذکر ہم ابتدا میں کرچکے ہیں، ان کی وجہ سے یہاں کا لینڈ اسکیپ اس طرح کا تخلیق ہوگیا تھا جس کے ایک طرف سمندر تھا، پانی کے چشمے تھے، پہاڑوں سے سمندر تک برساتی پانی کی ندیاں بہتیں اور یہاں گھنے جنگلات موجود تھے۔ ان تمام عناصر کی وجہ سے یہاں ایک ایسے ماحول نے جنم لیا جہاں میٹھے پانی، درختوں اور خوراک کی کمی نہیں تھی بالخصوص مچھلی اور دوسرے جانوروں کی بڑی تعداد یہاں موجود تھی۔\\n\\nعبدالرؤف خان کہتے ہیں کہ چونکہ آج کے دور میں ہزاروں برس کا لینڈ اسکیپ تبدیل ہوگیا ہے مگر پھر بھی اس پہاڑی سلسلے پر ہزاروں خاندان اپنی زندگی گزار سکتے ہیں اور گزار بھی رہے ہیں۔ کراچی کے اطراف میں اب تک جو قدیم بستیاں ریکارڈ ہوئی ہیں اُن میں امیلانو، اورنگی، منگھوپیر، گجرو، نل بازار (الھڈنو بستی)، لسبیلو اور گُجو شامل ہیں جنہیں ’نیولیتھک دور‘ (2200 سے 1000 قبل از مسیح) کی قدامت کے حوالے سے پہچانا جاتا ہے۔\\n\\nڈاکٹر عبدالرؤف کی تحقیقی رپورٹ میں موجود نقشہ\\n\\nڈاکٹر عبدالرؤف ایک بستی ’گزکل‘ کا بھی ذکر کرتے ہیں جو کراچی سے سیوہن جانے والے قدیم راستے پر امیلانو سے فقط 5 میل اور الھڈنو بستی سے 3 میل کے فاصلے پر ہے۔ اس بستی کے قریب ایک طویل گبر بند (گبربند اُس خاص پشتے کو کہا جاتا ہے جو ارتقائی انسان پتھروں سے اس لیے بناتے تھے تاکہ پہاڑیوں سے آنے والے برساتی پانی کو روک کر اُس پانی کو اپنی مرضی سے استعمال کیا جا سکے، خاص طور پر زراعت کے لیے اور پانی کو طویل عرصے محفوظ رکھنے کے لیے ایسا کیا جاتا تھا۔ اس طرح کے قدیم پشتے کراچی سے جیکب آباد تک دیکھنے کو مل جاتے ہیں۔ انہیں ایران کے زرتشتیوں سے جوڑا جاتا ہے کیونکہ انڈس ویلی اور ایران کے آپس میں انتہائی قدیم اور دلچسپ رابطے رہے ہیں بلکہ ایک زمانے میں سندھ ایران کا صوبہ بھی رہا ہے۔ گبربند (پشتہ) ایک دلچسپ ایجاد تھی جس سے متعلق ہم آنے والے دنوں میں ضرور تفصیل سے بات کریں گے) کا ذکر کیا ہے جو ’تڑ واری‘ برساتی بہاؤ پر بنایا گیا تھا۔ ڈاکٹر صاحب کراچی کے قرب و جوار میں 6 ایسے پشتوں کی بات کرتے ہیں۔ کچھ محققین کا کہنا ہے کہ ان کی تعداد 6 سے زائد ہے۔\\n\\nڈاکٹر عبدالرؤف کی تحقیق میں کھدائی کے دوران ملنے والے اوزار\\n\\nکھدیجی آبشاروں کے اوپر ایک قدیم انسانی بستی ہے جہاں پتھروں کی ایک دیوار موجود ہے جس کو مقامی لوگ کوٹاری (کوٹ والی) مقام کے نام سے بلاتے ہیں۔ دوسری بستی کونکر میں تھدھو ندی کے قریب کارو جبل کے پاس ہے۔ مگر کوٹاری مقام کی اپنی ایک الگ اہمیت ہے۔ یہ چھوٹی سی بستی اُس پہاڑی پر تھی جہاں سے کھدیجی وادی پر دُور دُور تک نظر رکھی جا سکتی ہے جبکہ اس مقام کے نیچے ایک خوبصورت آبشار ہے جس کا پانی وادی سے بہتا آگے جا کر مول ندی میں مل جاتا ہے۔ یہ بات بھی ممکن ہے کہ پرانے زمانے میں اس آبشار میں کافی پانی بہتا ہو اور بہتا ہوا ڈملوٹی تک پہنچتا ہو کیونکہ ملیر میں ڈملوٹی سے نکلنے والے پانی کا مرکزی مقام کھدیجی ہی رہا ہے۔\\n\\nکھدیجی کی پہاڑیاں اور وادیوں کو ہم ارتقائی انسان کی بستیوں میں شامل کرتے ہیں\\n\\nکھدیجی کی پہاڑیاں اور وادیاں وادی سندھ کی تہذیب سے پہلے کے آثار ہیں جس کو ہم ارتقائی انسان کی بستیوں میں شامل کرتے ہیں اس لیے کیونکہ وہاں وہ سارے اسباب موجود تھے جن میں ارتقائی انسان پنپ سکتا تھا جیسے شب و روز گزارنے کے لیے غاریں جو زیادہ ٹھنڈ، بارش، شدید گرمی، جانوروں کے حملے سے تحفظ فراہم کرتی تھیں، پانی کی فراوانی مطلب خوراک اور تحفظ فطری طور پر وہاں موجود تھا اس لیے اس کو یقینی طور پر نیولیتھک زمانے کی قدیم بستی کا اعزاز دے سکتے ہیں۔\\n\\nمحترم عبدالرؤف خان سندھ اور بلوچستان کے جنوبی حصے پر اور بھی تفصیل سے باتیں کرتا ہے جو دلچسپی سے خالی نہیں ہیں وہ میگالتھک (اس سے مراد وہ بڑے پتھر ہیں جو مرنے والوں کی یاد میں گاڑے گئے یا پتھریلا اسٹرکچر تعمیر کیا یا بنایا گیا، یہ انسان کی ترقی کی ایک اور منزل تھی جس کا زمانہ 2500 قبل از مسیح سے 200ء تک مانا جاتا ہے) بھاری پتھروں کی قبروں کی بھی بات کرتا ہے جو کراچی کے آس پاس ہیں اور جنوبی ہندوستان کے تسلسل سے تعلق رکھتے ہیں۔ اس سلسلے میں وہ مول وادی کی کچھ محفوظ قبروں کا ذکر کرتا ہے جنہیں بھاری لمبے پتھر سے ڈھانپا گیا ہے ان میں سے کچھ قبریں 8 فٹ تک لمبی ہیں۔\\n\\nعبدالرؤف خان کے مطابق یہ ڈملوٹی والی پتھروں کے زمانے کی قبروں سے مختلف ہیں اور ساتھ میں وہ حب وادی میں بڑے بھاری پتھروں سے بنائے ہوئے دائروں کی بھی بات کرتا ہے۔ ہمارے نصیب شاید اچھے ہیں کہ میگالتھک زمانے کے سندھ اور بلوچستان کی ان یادگاروں پر تحقیقی کام کرنے والے محترم ذوالفقار کہلوڑو صاحب بھی اس سفر میں ہمارے ساتھ شامل ہیں۔ ڈاکٹر عبدالرؤف صاحب اور بیگی صاحب کی تحقیق کے بعد ہم کلہوڑو صاحب سے اس زمانے کے متعلق ضرور تفصیلی بات کریں گے کیونکہ جس تسلسل کے ساتھ ہم آگے بڑھ رہے ہیں اُس میں یہ انتہائی اہم ہے کہ ہم انسانی ترقی کے ہر زمانے کے متعلق جانتے ہوئے آگے بڑھیں۔\\n\\nڈاکٹر عبدالرؤف خان کی تحقیقی رپورٹ میں کراچی ریجن کے قبلِ تاریخ دور سے پہلے کے مقامات\\n\\nسمندر کے ساحلی حصے اور قدیم پتھر کے زمانے پر بحث کرتے ہوئے ڈاکٹر صاحب جغرافیائی تبدیلیوں کا ذکر کرنا انتہائی ضرور سمجھتے ہیں۔ اُس کی مطالعہ اور تحقیق کے مطابق دلچسپ اور اہم بات اس خطے میں مائیکرولیتھس کی موجودگی ہے جن کا تعلق پہاڑیوں کی وادیوں اور اُن میں پانی کی چھوٹی چھوٹی گہرائیوں میں سے اُڑتی ہوئی پُرانی ریت سے ہے۔ جھیلوں کی طرح چھوٹے چھوٹے بہاؤ کی یہ گہری نشانیاں آخری برفانی زمانے کی بھی ہوسکتی ہیں۔\\n\\nمیسولیتھک اور آخری پیلیولیتھک دور کی ہمیں جو بستیاں ملی ہیں وہ گڈانی سے دھابیجی اور جنگشاہی تک پھیلی ہوئی ہیں۔ ان میں پہاڑیوں سے بہنے والے بہاؤ، کَلو، رن پٹیانی، کھدیجی، مول، تھدھو اور دوسری بارشی ندیوں کے قریب بہت بستیاں ملتی ہیں۔ ایک بستی یونیورسٹی کیمپس کے قریب تھی اور ایسی بستیوں کا سلسلہ حب ڈیم سے منگھوپیر پہاڑیوں تک پھیلا ہوا ہے ان پہاڑیوں میں اب بھی 12 سے زائد پانی کے چشمے ہیں جن میں سے کچھ خشک ہوچکے ہیں۔\\n\\nان بستیوں سے جو اوزار ملے ہیں اُن کے متعلق سماجی ارتقا کی بنیادوں پر آرکیالوجی کے ماہرین یہ رائے دیتے ہیں کہ سندھ اور بلوچستان کے میسولیتھک یقینی طور پر مچھلی اور جانوروں کے شکاری تھے، سمندر کے کناروں پر انہیں بھوک مٹانے کے لیے بہت سی آبی خوراک باآسانی سے مل جاتی ہوں گی جیسے سیپیاں، کچھوے، کیکڑے وغیرہ۔ ممکن ہے کہ اور بھی دیگر آبی مخلوقات بھی انہیں باآسانی کناروں پر سے مل جاتی ہوں جو اُن کے لیے پروٹین سے بھری شاندار خوراک ثابت ہوتی ہوں گی اور کم پانی والی جھیلوں سے وہ اپنی خوراک کے لیے مچھلیاں پکڑ لیتے ہوں گے۔\\n\\nسندھ اور بلوچستان کے میسولیتھک یقینی طور پر مچھلی اور جانوروں کے شکاری تھے\\n\\nاس کے علاوہ کچھ بستیوں سے مختلف جانوروں کی ہڈیاں بھی دریافت ہوئی ہیں۔ اور ان حقائق کی وجہ سے یہ امکان بھی ظاہر کیا جارہا ہے کہ شکار کے بہت سے ہتھیار لکڑیوں سے بنائے گئے ہوں گے۔ چونکہ ان ہتھیاروں کا ملنا انتہائی مشکل ہے اور اگر یہ مل جاتے ہیں تو یہ کسی معجزے سے کم نہیں ہوگا۔\\n\\nارتقائی انسان کے زمانے پر تحقیق کرنے والے محقق تاج صحرائی کہتے ہیں کہ ’سندھو دریا اور بہت زیادہ بارشوں کی وجہ سے یہاں گھنے جنگل اور قدآور درخت تھے کیونکہ اُس وقت کے معروضی حالات میں ممکن تھا۔ جنگلی بھینسیں، چیتے، جنگلی ہرنوں کی مختلف اقسام تھیں جن کے فوسلز (فوسلز پودوں اور جانوروں کی محفوظ باقیات کو کہا جاتا ہے جو ریت اور کیچڑ، قدیم سمندروں، جھیلوں اور دریاؤں کے نیچے دب جانے اور مخصوص ماحول ملنے سے پتھر میں تبدیل ہوجاتے ہیں۔ ان کی کم سے کم عمر بھی 10 ہزار قبل از مسیح مانی جاتی ہے) ملے ہیں۔\\n\\nموجودہ دور میں ان ہتھیاروں کا ملنا انتہائی مشکل ہیں جوکہ شکار کے لیے استعمال ہوتے تھے\\n\\n’بالکل ایسے دریاؤں میں ڈولفن اور گھڑیال بھی رہتے تھے۔ دریاؤں اور جھیلوں میں مچھلیاں بہت تھیں تو یہاں کناروں پر بسنے والوں کو فش ایٹرز کہا جاتا ہے۔ میرے پاس گندم کے خوشے کا فوسلز موجود ہے۔ بلکہ میرے پاس قدیم زمانے کی حیات کے فوسلز کی ایک وسیع کلیکشن موجود ہے‘۔\\n\\nڈاکٹر صاحب آخری پیلیولیتھک دور کے اوزاروں (آخری پیلیولیتھک دور، نیولیتھک دور کی ابتدا پر ختم ہوا) کے متعلق کہتے ہیں کہ دو انچ سے کم مگر ایک تیز دھار چُھری جیسا کاٹنے کا اوزار کھدیجی وادی سے ملا ہے اور حب کے قریب بھی ایسے اوزار دریافت ہوئے ہیں جس سے اندازہ ہوتا ہے کہ پیلیولیتھک بستیوں میں سے کچھ بستیوں پر میسولیتھک زمانے کے لوگ آباد رہے ہوں گے۔\\n\\nسندھو دریا کی پرانی بستیوں میں فوسلز بھی ملے ہیں\\n\\nکچھ بستیوں میں ان کے رہنے کی نشانیاں نہیں ملتیں۔ مگر کونکر، لانڈھی کے جنوب میں، واگھو ڈر (ریڑھی میان) کے قلعے کی دیوار سے جو نوکدار اور تیز دھار والے تیر کی طرح ہتھیار ملا ہے وہ میسولیتھک دور کے ہتھیاروں سے بالکل الگ ہے۔ جس سے لگتا ہے کہ یہ آخری پلائسٹوسین کے زمانے کا ہے۔ اس لیے یہ کہا جا سکتا ہے کہ اس جگہ پر انسانی سرگرمی اس سے بھی قدیم رہی ہوگی۔\\n\\nڈاکٹر صاحب کی طویل تحقیقی رپورٹ سے آپ نے اہم باتیں ملاحظہ کیں۔ یہ ایک انتہائی حیرت انگیز تحقیق تھی۔ اس اہم تحقیق پر پائلو بیگی نے مختلف ادوار پر ترقی یافتہ سائنسی بنیادوں پر تحقیق کی جو اب میں آپ کو بتانے جارہا ہوں۔ بیگی صاحب کی تحقیق کا مطالعہ کرنے کے بعد آپ یقیناً حیران ہوں گے۔ مگر اس رپورٹ سے پہلے اپنے ذہن سے موجودہ کراچی کی تصویر کو بالکل مٹادیں کیونکہ اگر موجودہ منظرنامہ آپ کے ذہن میں رہا تو گزرے زمانوں کو سمجھنے میں ضرور مشکل ہوگی۔\\n\\nگزشتہ حصوں میں ہم نے اُن زمانوں کے اسکیچ اس لیے شیئر کیے تھے تاکہ ہم اُن زمانوں کے لینڈ اسکیپ کے منظرنامے آپ کے ذہن میں بُن سکیں۔ یہ تو ہزاروں برسوں کی کتھا ہے آپ سوچیں جب آپ فقط 100 برس سے بھی کم وقت پرانی عبداللہ شاہ غازی کے مزار کی ایک بلیک اینڈ وائٹ تصویر دیکھتے ہیں جہاں ایک ٹیلے پر درگاہ بنی ہوئی ہے اور وہ دیکھ کر ہم سب حیران رہ جاتے ہیں کہ ایک سو برس میں اتنی تبدیلی کیسے آگئی؟\\n\\nمیسولتھک دور میں انسانی بستیاں\\n\\nمگر چونکہ ماضی کے مقابلے میں حال میں انسان میں ہر حوالے سے تیزی آئی ہے۔ مگر اس تیزی کی بنیادیں کم سے کم بھی 25 ہزار برس قبل مسیح کے پُرخطر، پُراسرار، پُرآشوب اور اچانک آجانے والے خطرات اور خوف سے بھری پڑی تھیں۔ کیا آپ سوچ سکتے ہیں کہ ایک زمانے میں کراچی کے ان ساحلوں پر Giant Mangrove کے گھنے جنگلات تھے یہ تمر کے جنگلات کی وہ نسل ہے جو دیو ہیکل کہلاتی ہے؟ آنے والے سفر میں ہم تمر کے اُن دیوہیکل درختوں کے جنگلات میں بھی جائیں گے۔\\n\\nحوالہ جات'}\n"
     ]
    }
   ],
   "source": [
    "# Access the 'train' subset\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Select and display the first row\n",
    "first_row = train_dataset[0]\n",
    "print(first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to merge all text into one large dataset\n",
    "def merge_all_text(dataset):\n",
    "    combined_text = \"\"\n",
    "    for example in dataset['train']:\n",
    "        combined_text += example['text'] + \" \"  # Add a space between articles\n",
    "    return combined_text.strip()\n",
    "\n",
    "# Merge all text from the dataset\n",
    "combined_text = merge_all_text(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اس سلسلے کی دیگر اقساط یہاں پڑھیے۔\n",
      "\n",
      "یہ کیسے ممکن ہے کہ کسی کا مُستقبل ماضی کے بغیر ہو؟ کیونکہ دن دن سے اور پل پل سے جُڑا ہوتا ہے۔ جس طرح ہم گزرے دن کو بھی کل کہتے ہیں اور آنے والے دن کو بھی کل کہتے ہیں، اس لیے کیونکہ کل اور کل میں جو آج ہے وہ ایک کڑی ہے جو دونوں کو آپس میں جوڑ کر رکھتی ہے۔ تو ہم اگر کبھی تنہائی میں کچھ پل نکال کر سوچیں کہ ہماری جتنی بھی زندگی گزری خواہ وہ 24 برس ہو، 64 کی یا 84 برس ہو، ہم یہ حیات اسی صورت میں گزار پاتے ہیں جب ہم اپنے گزرے برسوں اور شب و روز سے جُڑے رہتے ہیں۔\n",
      "\n",
      "اگر لوگوں، قوموں اور تہذیبوں سے اُن کا ماضی چھین لیا جائے تو وہ شاید زیادہ عرصہ زندہ نہیں رہ پائیں گی۔ اُن میں آگے بڑھنے کی چاہت ختم ہوجائے گی، تگ و دو کا جوہر ان سے چھن جائے گا بالکل ایسے جیسے کسی وجود کو الزائمر کی بیماری لگتی ہے اور اُس کی یادداشت کے خلیے بے جان ہونے لگتے ہیں۔ آپ یہ سمجھیں کہ یہ بیماری اُس سے اُس کا ماضی چھین لیتی ہے۔ اور وہ اِسی کیفیت میں زیادہ زندگی نہیں جی سکے گا اور اگر جیے گا بھی تو حیات کے اُن رنگوں، ذائقوں اور کیفیتوں کے ریگستان میں جہاں موت کی ویرانی کے سوا کچھ نہیں بچ\n"
     ]
    }
   ],
   "source": [
    "# Print the first few lines of the merged text\n",
    "print(combined_text[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Word Count: 247616\n"
     ]
    }
   ],
   "source": [
    "# Function to count the number of words\n",
    "def count_words(text):\n",
    "    words = text.split()  \n",
    "    return len(words)\n",
    "\n",
    "# Count the words in the merged text\n",
    "word_count = count_words(combined_text)\n",
    "\n",
    "# Print the total word count\n",
    "print(\"Total Word Count:\", word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اس سلسلے کی دیگر اقساط یہاں پڑھیے۔\n",
      "\n",
      "یہ کیسے ممکن ہے کہ کسی کا مستقبل ماضی کے بغیر ہو کیونکہ دن دن سے اور پل پل سے جڑا ہوتا ہے۔ جس طرح ہم گزرے دن کو بھی کل کہتے ہیں اور آنے والے دن کو بھی کل کہتے ہیں اس لیے کیونکہ کل اور کل میں جو آج ہے وہ ایک کڑی ہے جو دونوں کو اپس میں جوڑ کر رکھتی ہے۔ تو ہم اگر کبھی تنہائی میں کچھ پل نکال کر سوچیں کہ ہماری جتنی بھی زندگی گزری خواہ وہ 24 برس ہو 64 کی یا 84 برس ہو ہم یہ حیات اسی صورت میں گزار پاتے ہیں جب ہم اپنے گزرے برسوں اور شب و روز سے جڑے رہتے ہیں۔\n",
      "\n",
      "اگر لوگوں قوموں اور تہذیبوں سے ان کا ماضی چھین لیا جائے تو وہ شاید زیادہ عرصہ زندہ نہیں رہ پائیں گی۔ ان میں آگے بڑھنے کی چاہت ختم ہوجائے گی تگ و دو کا جوہر ان سے چھن جائے گا بالکل ایسے جیسے کسی وجود کو الزائمر کی بیماری لگتی ہے اور اس کی یادداشت کے خلیے بے جان ہونے لگتے ہیں۔ آپ یہ سمجھیں کہ یہ بیماری اس سے اس کا ماضی چھین لیتی ہے۔ اور وہ اسی کیفیت میں زیادہ زندگی نہیں جی سکے گا اور اگر جیے گا بھی تو حیات کے ان رنگوں ذائقوں اور کیفیتوں کے ریگستان میں جہاں موت کی ویرانی کے سوا کچھ نہیں بچتا۔ نہ رنگوں کے پھ\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Function to remove all punctuation except for Urdu period (۔)\n",
    "def remove_punctuation(text):\n",
    "    # Regex pattern to remove all punctuation except for \"۔\"\n",
    "    pattern = r'[^\\w\\s۔]'  # \\w matches word characters, \\s matches spaces, \"۔\" is kept\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# Remove all punctuation except for \"۔\"\n",
    "cleaned_text = remove_punctuation(combined_text)\n",
    "\n",
    "print(cleaned_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentence Count: 9251\n"
     ]
    }
   ],
   "source": [
    "# Function to count the number of sentences based on the period \"۔\"\n",
    "def count_sentences(text):\n",
    "    sentences = text.split('۔') \n",
    "    # Filter out empty strings resulting from splitting\n",
    "    return len([s for s in sentences if s.strip()])  \n",
    "\n",
    "# Get the sentences in the cleaned text\n",
    "sentences = count_sentences(cleaned_text) \n",
    "\n",
    "# Print the total sentence count\n",
    "print(\"Total Sentence Count:\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split sentences based on \"۔\" and remove those with length <= 3\n",
    "def filter_short_sentences(text):\n",
    "    # Split the text by \"۔\"\n",
    "    sentences = text.split('۔')\n",
    "    # Filter out sentences with length <= 3\n",
    "    filtered_sentences = [s.strip() for s in sentences if len(s.strip()) > 3]\n",
    "    # Join the filtered sentences back with \"۔\" to get the cleaned text\n",
    "    cleaned_text = '۔'.join(filtered_sentences)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage\n",
    "cleaned_text = filter_short_sentences(cleaned_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sentences of length 3 or less found.\n"
     ]
    }
   ],
   "source": [
    "# Function to verify there are no sentences with length 3 or less\n",
    "def check_short_sentences(text):\n",
    "    # Split the text by \"۔\"\n",
    "    sentences = text.split('۔')\n",
    "    # Find sentences with length <= 3\n",
    "    short_sentences = [s.strip() for s in sentences if len(s.strip()) <= 3]\n",
    "    \n",
    "    if short_sentences:\n",
    "        print(\"Found short sentences:\", short_sentences)\n",
    "    else:\n",
    "        print(\"No sentences of length 3 or less found.\")\n",
    "\n",
    "# Check the cleaned_text for any short sentences\n",
    "check_short_sentences(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentence Count: 9227\n"
     ]
    }
   ],
   "source": [
    "# Function to count the number of sentences based on the period \"۔\"\n",
    "def count_sentences(text):\n",
    "    sentences = text.split('۔') \n",
    "    # Filter out empty strings resulting from splitting\n",
    "    return len([s for s in sentences if s.strip()])  \n",
    "\n",
    "# Get the sentences in the cleaned text\n",
    "sentences = count_sentences(cleaned_text) \n",
    "\n",
    "# Print the total sentence count\n",
    "print(\"Total Sentence Count:\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اس سلسلے کی دیگر اقساط یہاں پڑھیےیہ کیسے ممکن ہے کہ کسی کا مستقبل ماضی کے بغیر ہو کیونکہ دن دن سے اور پل پل سے جڑا ہوتا ہےجس طرح ہم گزرے دن کو بھی کل کہتے ہیں اور آنے والے دن کو بھی کل کہتے ہیں اس لیے کیونکہ کل اور کل میں جو آج ہے وہ ایک کڑی ہے جو دونوں کو اپس میں جوڑ کر رکھتی ہےتو ہم اگر کبھی تنہائی میں کچھ پل نکال کر سوچیں کہ ہماری جتنی بھی زندگی گزری خواہ وہ 24 برس ہو 64 کی یا 84 برس ہو ہم یہ حیات اسی صورت میں گزار پاتے ہیں جب ہم اپنے گزرے برسوں اور شب و روز سے جڑے رہتے ہیںاگر لوگوں قوموں اور تہذیبوں سے ان کا ماضی چھین لیا جائے تو وہ شاید زیادہ عرصہ زندہ نہیں رہ پائیں گیان میں آگے بڑھنے کی چاہت ختم ہوجائے گی تگ و دو کا جوہر ان سے چھن جائے گا بالکل ایسے جیسے کسی وجود کو الزائمر کی بیماری لگتی ہے اور اس کی یادداشت کے خلیے بے جان ہونے لگتے ہیںآپ یہ سمجھیں کہ یہ بیماری اس سے اس کا ماضی چھین لیتی ہےاور وہ اسی کیفیت میں زیادہ زندگی نہیں جی سکے گا اور اگر جیے گا بھی تو حیات کے ان رنگوں ذائقوں اور کیفیتوں کے ریگستان میں جہاں موت کی ویرانی کے سوا کچھ نہیں بچتانہ رنگوں کے پھول کھلتے ہیں نہ ذائقوں کی خوشبو کا کوئی جھونکا آتا ہے اور نہ غم و خوشی کی کوئی فاختہ اڑتی ہے اور نہ آسمان کی نیلاہٹ کے گہرے رنگ میں کشش کی کوئی ناؤ چلتی ہےبس ایک بے رنگ اور بے ذائقہ حیاتہمارا آج کا سفر بھی سمندر کنارے اور ان پہاڑیوں کے جنگلات آبشاروں غاروں اور قدامت کی ان پگڈنڈیوں پر ہی ہے مگر اج ہمیں ڈاکٹر عبدالرؤف کی کراچی ریجن پر کی گئی تحقیق اور اس تحقیق پر پاؤلو بیگی کی تازہ تحقیق پر تفصیلی بات کرنی تھی لیکن جیسے ہی ہم سفر کرنے کی تیاری میں تھے تو ہمارے ساتھیوں میں سے کسی نے دو تین سوال کر ڈالے کہ کھیرتھر کے\n"
     ]
    }
   ],
   "source": [
    "# Function to remove the Urdu period \"۔\" from the text\n",
    "def remove_period(text):\n",
    "    cleaned_text_without_period = text.replace('۔', '')  \n",
    "    return cleaned_text_without_period\n",
    "\n",
    "cleaned_text_without_period = remove_period(cleaned_text)\n",
    "\n",
    "print(cleaned_text_without_period[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule Based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected Sentences:\n",
      "اس سلسلے کی دیگر اقساط یہاں پڑھیےیہ کیسے ممکن ہے\n",
      "کہ کسی کا مستقبل ماضی کے بغیر ہو کیونکہ دن دن سے اور پل پل سے جڑا ہوتا ہےجس طرح ہم گزرے دن کو بھی کل کہتے ہیں اور آنے والے دن کو بھی کل کہتے ہیں اس لیے کیونکہ کل اور کل میں جو آج ہے\n",
      "وہ ایک کڑی ہے\n",
      "جو دونوں کو اپس میں جوڑ کر رکھتی ہےتو ہم اگر کبھی تنہائی میں کچھ پل نکال کر سوچیں کہ ہماری جتنی بھی زندگی گزری خواہ وہ 24 برس ہو 64 کی یا 84 برس ہو ہم یہ حیات اسی صورت میں گزار پاتے ہیں جب ہم اپنے گزرے برسوں اور شب و روز سے جڑے رہتے ہیںاگر لوگوں قوموں اور تہذیبوں سے ان کا ماضی چھین لیا جائے تو وہ شاید زیادہ عرصہ زندہ نہیں\n",
      "رہ پائیں گیان میں آگے بڑھنے کی چاہت ختم ہوجائے گی\n",
      "تگ و دو\n",
      "کا جوہر ان سے چھن جائے گا\n",
      "بالکل ایسے جیسے کسی وجود کو الزائمر کی بیماری لگتی ہے\n",
      "اور اس کی یادداشت کے خلیے بے جان ہونے لگتے ہیںآپ یہ سمجھیں کہ یہ بیماری اس سے اس کا ماضی چھین لیتی ہےاور وہ اسی کیفیت میں زیادہ زندگی نہیں\n",
      "جی سکے گا\n"
     ]
    }
   ],
   "source": [
    "# List of Urdu sentence-end words\n",
    "urdu_sentence_end_words = [\n",
    "    \"ہے\", \"تھا\", \"تھی\", \"تھے\", \"کیا\", \"گی\", \"گا\", \"گے\", \"چاہیے\", \"رہا\", \"رہی\", \"ہوں\", \n",
    "    \"جاتا\", \"جاتی\", \"آیا\", \"آئی\", \"جا رہا\", \"جا رہی\", \"سکتا\", \"سکتی\", \"دو\", \"کرنا\", \n",
    "    \"دینا\", \"پڑتا\", \"پڑتی\", \"لو\", \"رکو\", \"نہیں\", \"گئے\", \"کیوں\", \"پھر\", \"نا\", \"ہے نا\"\n",
    "]\n",
    "# Function to detect sentence boundaries using a rule-based approach\n",
    "def detect_sentence_boundaries_rule_based(cleaned_text_without_period, urdu_sentence_end_words):\n",
    "    # Split the text into words\n",
    "    words = cleaned_text_without_period.split()\n",
    "    \n",
    "    # Initialize a list to store sentence boundaries (list of sentences)\n",
    "    detected_sentences = []\n",
    "    current_sentence = []\n",
    "    \n",
    "    # Loop through the words to identify sentence boundaries\n",
    "    for word in words:\n",
    "        current_sentence.append(word)\n",
    "        if word in urdu_sentence_end_words:\n",
    "            # Sentence boundary detected, join the current sentence words and add to list\n",
    "            detected_sentences.append(' '.join(current_sentence))\n",
    "            current_sentence = []\n",
    "    \n",
    "    # If there's any remaining text, add it as the last sentence\n",
    "    if current_sentence:\n",
    "        detected_sentences.append(' '.join(current_sentence))\n",
    "    \n",
    "    return detected_sentences\n",
    "\n",
    "detected_sentences = detect_sentence_boundaries_rule_based(cleaned_text_without_period, urdu_sentence_end_words)\n",
    "\n",
    "# Print only a few detected sentences\n",
    "print(\"\\nDetected Sentences:\")\n",
    "for sentence in detected_sentences[:10]:  # Change 5 to however many sentences you want to print\n",
    "    print(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentences:\n",
      "1: اس سلسلے کی دیگر اقساط یہاں پڑھیے\n",
      "2: یہ کیسے ممکن ہے کہ کسی کا مستقبل ماضی کے بغیر ہو کیونکہ دن دن سے اور پل پل سے جڑا ہوتا ہے\n",
      "3: جس طرح ہم گزرے دن کو بھی کل کہتے ہیں اور آنے والے دن کو بھی کل کہتے ہیں اس لیے کیونکہ کل اور کل میں جو آج ہے وہ ایک کڑی ہے جو دونوں کو اپس میں جوڑ کر رکھتی ہے\n",
      "4: تو ہم اگر کبھی تنہائی میں کچھ پل نکال کر سوچیں کہ ہماری جتنی بھی زندگی گزری خواہ وہ 24 برس ہو 64 کی یا 84 برس ہو ہم یہ حیات اسی صورت میں گزار پاتے ہیں جب ہم اپنے گزرے برسوں اور شب و روز سے جڑے رہتے ہیں\n",
      "5: اگر لوگوں قوموں اور تہذیبوں سے ان کا ماضی چھین لیا جائے تو وہ شاید زیادہ عرصہ زندہ نہیں رہ پائیں گی\n",
      "6: ان میں آگے بڑھنے کی چاہت ختم ہوجائے گی تگ و دو کا جوہر ان سے چھن جائے گا بالکل ایسے جیسے کسی وجود کو الزائمر کی بیماری لگتی ہے اور اس کی یادداشت کے خلیے بے جان ہونے لگتے ہیں\n",
      "7: آپ یہ سمجھیں کہ یہ بیماری اس سے اس کا ماضی چھین لیتی ہے\n",
      "8: اور وہ اسی کیفیت میں زیادہ زندگی نہیں جی سکے گا اور اگر جیے گا بھی تو حیات کے ان رنگوں ذائقوں اور کیفیتوں کے ریگستان میں جہاں موت کی ویرانی کے سوا کچھ نہیں بچتا\n",
      "9: نہ رنگوں کے پھول کھلتے ہیں نہ ذائقوں کی خوشبو کا کوئی جھونکا آتا ہے اور نہ غم و خوشی کی کوئی فاختہ اڑتی ہے اور نہ آسمان کی نیلاہٹ کے گہرے رنگ میں کشش کی کوئی ناؤ چلتی ہے\n"
     ]
    }
   ],
   "source": [
    "# just for checking\n",
    "\n",
    "# Function to extract original sentences\n",
    "def extract_original_sentences(cleaned_text):\n",
    "    # Split original text by punctuation (assuming '۔' is the sentence boundary marker in original text)\n",
    "    original_sentences = cleaned_text.split('۔')\n",
    "\n",
    "    # Filter out empty sentences caused by splitting\n",
    "    original_sentences = [sentence.strip() for sentence in original_sentences if sentence.strip()]\n",
    "    \n",
    "    return original_sentences\n",
    "\n",
    "# Extract the original sentences\n",
    "original_sentences = extract_original_sentences(cleaned_text)\n",
    "\n",
    "# Print a few original sentences for clarity\n",
    "print(\"Original Sentences:\")\n",
    "for i, sentence in enumerate(original_sentences):\n",
    "    if i < 9:  # Print only the first 5 sentences\n",
    "        print(f\"{i + 1}: {sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence count (with punctuation): 9227\n",
      "Detected sentence count (rule-based): 9470\n"
     ]
    }
   ],
   "source": [
    "# Compare original and rule-based sentences\n",
    "def compare_sentence_counts(cleaned_text, detected_sentences):\n",
    "    # Split original text by punctuation (assuming '۔' is the sentence boundary marker in original text)\n",
    "    original_sentences = cleaned_text.split('۔')\n",
    "\n",
    "    # Filter out empty sentences caused by splitting\n",
    "    original_sentences = [sentence.strip() for sentence in original_sentences if sentence.strip()]\n",
    "\n",
    "    # Compare the number of sentences in both versions\n",
    "    original_count = len(original_sentences)\n",
    "    detected_count = len(detected_sentences)\n",
    "\n",
    "    print(f\"Original sentence count (with punctuation): {original_count}\")\n",
    "    print(f\"Detected sentence count (rule-based): {detected_count}\")\n",
    "\n",
    "    return original_count, detected_count\n",
    "\n",
    "\n",
    "# Compare the number of sentences\n",
    "original_count, detected_count = compare_sentence_counts(cleaned_text, detected_sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(original_sentences, detected_sentences):\n",
    "    # Convert the lists of sentences to sets for easier comparison\n",
    "    original_set = set(original_sentences)\n",
    "    detected_set = set(detected_sentences)\n",
    "    \n",
    "    # Calculate True Positives, False Positives, and False Negatives\n",
    "    true_positives = len(original_set & detected_set)\n",
    "    false_positives = len(detected_set - original_set)\n",
    "    false_negatives = len(original_set - detected_set)\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Compute metrics\n",
    "precision, recall, f1 = compute_metrics(original_sentences, detected_sentences)\n",
    "\n",
    "# Print results\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save sentences to a CSV file\n",
    "def save_sentences_to_csv(sentences, filename):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for sentence in sentences:\n",
    "            writer.writerow([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save both lists to separate CSV files\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m \u001b[43msave_sentences_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_sentences.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m save_sentences_to_csv(detected_sentences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36msave_sentences_to_csv\u001b[1;34m(sentences, filename)\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_sentences_to_csv\u001b[39m(sentences, filename):\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;32m----> 4\u001b[0m         writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mwriter(file)\n",
      "\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "\u001b[0;32m      6\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([sentence])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Save both lists to separate CSV files\n",
    "save_sentences_to_csv(original_sentences, \"original_sentences.csv\")\n",
    "save_sentences_to_csv(detected_sentences, \"detected_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save both lists to separate CSV files\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m \u001b[43msave_sentences_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_sentences.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m save_sentences_to_csv(detected_sentences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36msave_sentences_to_csv\u001b[1;34m(sentences, filename)\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_sentences_to_csv\u001b[39m(sentences, filename):\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;32m----> 4\u001b[0m         writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mwriter(file)\n",
      "\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "\u001b[0;32m      6\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([sentence])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Save both lists to separate CSV files\n",
    "save_sentences_to_csv(original_sentences, \"original_sentences.csv\")\n",
    "save_sentences_to_csv(detected_sentences, \"detected_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save both lists to separate CSV files\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m \u001b[43msave_sentences_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_sentences.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m save_sentences_to_csv(detected_sentences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36msave_sentences_to_csv\u001b[1;34m(sentences, filename)\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_sentences_to_csv\u001b[39m(sentences, filename):\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;32m----> 4\u001b[0m         writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mwriter(file)\n",
      "\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "\u001b[0;32m      6\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([sentence])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Save both lists to separate CSV files\n",
    "save_sentences_to_csv(original_sentences, \"original_sentences.csv\")\n",
    "save_sentences_to_csv(detected_sentences, \"detected_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save both lists to separate CSV files\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m \u001b[43msave_sentences_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_sentences.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m save_sentences_to_csv(detected_sentences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36msave_sentences_to_csv\u001b[1;34m(sentences, filename)\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_sentences_to_csv\u001b[39m(sentences, filename):\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;32m----> 4\u001b[0m         writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mwriter(file)\n",
      "\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "\u001b[0;32m      6\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([sentence])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Save both lists to separate CSV files\n",
    "save_sentences_to_csv(original_sentences, \"original_sentences.csv\")\n",
    "save_sentences_to_csv(detected_sentences, \"detected_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save both lists to separate CSV files\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m \u001b[43msave_sentences_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_sentences.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m save_sentences_to_csv(detected_sentences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36msave_sentences_to_csv\u001b[1;34m(sentences, filename)\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_sentences_to_csv\u001b[39m(sentences, filename):\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;32m----> 4\u001b[0m         writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mwriter(file)\n",
      "\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "\u001b[0;32m      6\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([sentence])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Save both lists to separate CSV files\n",
    "save_sentences_to_csv(original_sentences, \"original_sentences.csv\")\n",
    "save_sentences_to_csv(detected_sentences, \"detected_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save both lists to separate CSV files\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m \u001b[43msave_sentences_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_sentences.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m save_sentences_to_csv(detected_sentences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36msave_sentences_to_csv\u001b[1;34m(sentences, filename)\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_sentences_to_csv\u001b[39m(sentences, filename):\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;32m----> 4\u001b[0m         writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mwriter(file)\n",
      "\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "\u001b[0;32m      6\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([sentence])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Save both lists to separate CSV files\n",
    "save_sentences_to_csv(original_sentences, \"original_sentences.csv\")\n",
    "save_sentences_to_csv(detected_sentences, \"detected_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save both lists to separate CSV files\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m \u001b[43msave_sentences_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_sentences.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m save_sentences_to_csv(detected_sentences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36msave_sentences_to_csv\u001b[1;34m(sentences, filename)\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_sentences_to_csv\u001b[39m(sentences, filename):\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;32m----> 4\u001b[0m         writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mwriter(file)\n",
      "\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "\u001b[0;32m      6\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([sentence])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Save both lists to separate CSV files\n",
    "save_sentences_to_csv(original_sentences, \"original_sentences.csv\")\n",
    "save_sentences_to_csv(detected_sentences, \"detected_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save both lists to separate CSV files\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m \u001b[43msave_sentences_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_sentences.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m save_sentences_to_csv(detected_sentences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36msave_sentences_to_csv\u001b[1;34m(sentences, filename)\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_sentences_to_csv\u001b[39m(sentences, filename):\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;32m----> 4\u001b[0m         writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mwriter(file)\n",
      "\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "\u001b[0;32m      6\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([sentence])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Save both lists to separate CSV files\n",
    "save_sentences_to_csv(original_sentences, \"original_sentences.csv\")\n",
    "save_sentences_to_csv(detected_sentences, \"detected_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save both lists to separate CSV files\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m \u001b[43msave_sentences_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_sentences.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m save_sentences_to_csv(detected_sentences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36msave_sentences_to_csv\u001b[1;34m(sentences, filename)\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_sentences_to_csv\u001b[39m(sentences, filename):\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;32m----> 4\u001b[0m         writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mwriter(file)\n",
      "\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "\u001b[0;32m      6\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([sentence])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Save both lists to separate CSV files\n",
    "save_sentences_to_csv(original_sentences, \"original_sentences.csv\")\n",
    "save_sentences_to_csv(detected_sentences, \"detected_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save both lists to separate CSV files\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m \u001b[43msave_sentences_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_sentences.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m save_sentences_to_csv(detected_sentences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36msave_sentences_to_csv\u001b[1;34m(sentences, filename)\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_sentences_to_csv\u001b[39m(sentences, filename):\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;32m----> 4\u001b[0m         writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mwriter(file)\n",
      "\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "\u001b[0;32m      6\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([sentence])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Save both lists to separate CSV files\n",
    "save_sentences_to_csv(original_sentences, \"original_sentences.csv\")\n",
    "save_sentences_to_csv(detected_sentences, \"detected_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save both lists to separate CSV files\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m \u001b[43msave_sentences_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_sentences.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m save_sentences_to_csv(detected_sentences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36msave_sentences_to_csv\u001b[1;34m(sentences, filename)\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_sentences_to_csv\u001b[39m(sentences, filename):\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;32m----> 4\u001b[0m         writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mwriter(file)\n",
      "\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "\u001b[0;32m      6\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([sentence])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Save both lists to separate CSV files\n",
    "save_sentences_to_csv(original_sentences, \"original_sentences.csv\")\n",
    "save_sentences_to_csv(detected_sentences, \"detected_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save both lists to separate CSV files\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m \u001b[43msave_sentences_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_sentences.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m save_sentences_to_csv(detected_sentences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36msave_sentences_to_csv\u001b[1;34m(sentences, filename)\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_sentences_to_csv\u001b[39m(sentences, filename):\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;32m----> 4\u001b[0m         writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mwriter(file)\n",
      "\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "\u001b[0;32m      6\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([sentence])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Save both lists to separate CSV files\n",
    "save_sentences_to_csv(original_sentences, \"original_sentences.csv\")\n",
    "save_sentences_to_csv(detected_sentences, \"detected_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save both lists to separate CSV files\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m \u001b[43msave_sentences_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_sentences.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m save_sentences_to_csv(detected_sentences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36msave_sentences_to_csv\u001b[1;34m(sentences, filename)\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_sentences_to_csv\u001b[39m(sentences, filename):\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;32m----> 4\u001b[0m         writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mwriter(file)\n",
      "\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "\u001b[0;32m      6\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([sentence])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Save both lists to separate CSV files\n",
    "save_sentences_to_csv(original_sentences, \"original_sentences.csv\")\n",
    "save_sentences_to_csv(detected_sentences, \"detected_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save both lists to separate CSV files\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m \u001b[43msave_sentences_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_sentences.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m save_sentences_to_csv(detected_sentences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36msave_sentences_to_csv\u001b[1;34m(sentences, filename)\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_sentences_to_csv\u001b[39m(sentences, filename):\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;32m----> 4\u001b[0m         writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mwriter(file)\n",
      "\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "\u001b[0;32m      6\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([sentence])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Save both lists to separate CSV files\n",
    "save_sentences_to_csv(original_sentences, \"original_sentences.csv\")\n",
    "save_sentences_to_csv(detected_sentences, \"detected_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save both lists to separate CSV files\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m \u001b[43msave_sentences_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_sentences.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m save_sentences_to_csv(detected_sentences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36msave_sentences_to_csv\u001b[1;34m(sentences, filename)\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_sentences_to_csv\u001b[39m(sentences, filename):\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;32m----> 4\u001b[0m         writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mwriter(file)\n",
      "\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "\u001b[0;32m      6\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([sentence])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Save both lists to separate CSV files\n",
    "save_sentences_to_csv(original_sentences, \"original_sentences.csv\")\n",
    "save_sentences_to_csv(detected_sentences, \"detected_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save both lists to separate CSV files\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m \u001b[43msave_sentences_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_sentences.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m save_sentences_to_csv(detected_sentences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36msave_sentences_to_csv\u001b[1;34m(sentences, filename)\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_sentences_to_csv\u001b[39m(sentences, filename):\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;32m----> 4\u001b[0m         writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mwriter(file)\n",
      "\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "\u001b[0;32m      6\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([sentence])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Save both lists to separate CSV files\n",
    "save_sentences_to_csv(original_sentences, \"original_sentences.csv\")\n",
    "save_sentences_to_csv(detected_sentences, \"detected_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save both lists to separate CSV files\u001b[39;00m\n",
      "\u001b[1;32m----> 2\u001b[0m \u001b[43msave_sentences_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moriginal_sentences.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      3\u001b[0m save_sentences_to_csv(detected_sentences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36msave_sentences_to_csv\u001b[1;34m(sentences, filename)\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_sentences_to_csv\u001b[39m(sentences, filename):\n",
      "\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "\u001b[1;32m----> 4\u001b[0m         writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mwriter(file)\n",
      "\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "\u001b[0;32m      6\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow([sentence])\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Save both lists to separate CSV files\n",
    "save_sentences_to_csv(original_sentences, \"original_sentences.csv\")\n",
    "save_sentences_to_csv(detected_sentences, \"detected_sentences.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using POS Tagging For SBD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 1.57MB/s]                    \n",
      "2024-10-27 23:52:57 INFO: Downloaded file to C:\\Users\\Kanza Nasim\\stanza_resources\\resources.json\n",
      "2024-10-27 23:52:57 INFO: Downloading default packages for language: ur (Urdu) ...\n",
      "2024-10-27 23:52:58 INFO: File exists: C:\\Users\\Kanza Nasim\\stanza_resources\\ur\\default.zip\n",
      "2024-10-27 23:53:00 INFO: Finished downloading models and saved to C:\\Users\\Kanza Nasim\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Download Urdu model\n",
    "stanza.download('ur')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 23:53:00 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 1.85MB/s]                    \n",
      "2024-10-27 23:53:01 INFO: Downloaded file to C:\\Users\\Kanza Nasim\\stanza_resources\\resources.json\n",
      "2024-10-27 23:53:01 INFO: Loading these models for language: ur (Urdu):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | udtb          |\n",
      "| pos       | udtb_nocharlm |\n",
      "=============================\n",
      "\n",
      "2024-10-27 23:53:01 INFO: Using device: cpu\n",
      "2024-10-27 23:53:01 INFO: Loading: tokenize\n",
      "2024-10-27 23:53:03 INFO: Loading: pos\n",
      "2024-10-27 23:53:04 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pipeline for Urdu language\n",
    "nlp = stanza.Pipeline('ur', processors='tokenize,pos', tokenize_pretokenized=False)\n",
    "\n",
    "# Process your text with the pipeline\n",
    "doc = nlp(cleaned_text_without_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: اس, POS: DET\n",
      "Text: سلسلے, POS: NOUN\n",
      "Text: کی, POS: ADP\n",
      "Text: دیگر, POS: ADJ\n",
      "Text: اقساط, POS: NOUN\n",
      "Text: یہاں, POS: PRON\n",
      "Text: پڑھیےیہ, POS: NOUN\n",
      "Text: کیسے, POS: PRON\n",
      "Text: ممکن, POS: ADJ\n",
      "Text: ہے, POS: VERB\n",
      "Text: کہ, POS: SCONJ\n",
      "Text: کسی, POS: PRON\n",
      "Text: کا, POS: ADP\n",
      "Text: مستقبل, POS: NOUN\n",
      "Text: ماضی, POS: NOUN\n",
      "Text: کے, POS: ADP\n",
      "Text: بغیر, POS: PART\n",
      "Text: ہو, POS: VERB\n",
      "Text: کیونکہ, POS: SCONJ\n",
      "Text: دن, POS: NOUN\n",
      "Text: دن, POS: NOUN\n",
      "Text: سے, POS: ADP\n",
      "Text: اور, POS: CCONJ\n",
      "Text: پل, POS: NOUN\n",
      "Text: پل, POS: NOUN\n",
      "Text: سے, POS: ADP\n",
      "Text: جڑا, POS: VERB\n",
      "Text: ہوتا, POS: VERB\n",
      "Text: ہےجس, POS: PRON\n",
      "Text: طرح, POS: NOUN\n"
     ]
    }
   ],
   "source": [
    "# Set limit for how many tagged words to print\n",
    "word_limit = 30\n",
    "word_count = 0\n",
    "\n",
    "# Iterate and print a limited number of POS-tagged words\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        if word_count < word_limit:\n",
    "            print(f'Text: {word.text}, POS: {word.pos}')\n",
    "            word_count += 1\n",
    "        else:\n",
    "            break\n",
    "    if word_count >= word_limit:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: اس سلسلے کی دیگر اقساط یہاں پڑھیےیہ کیسے ممکن ہے کہ کسی کا مستقبل ماضی کے بغیر ہو کیونکہ دن دن سے اور پل پل سے جڑا ہوتا ہےجس طرح ہم گزرے دن کو بھی کل کہتے ہیں\n",
      "Sentence 2: اور آنے والے دن کو بھی کل کہتے ہیں\n",
      "Sentence 3: اس لیے کیونکہ کل اور کل میں جو آج ہے\n",
      "Sentence 4: وہ ایک کڑی ہے\n",
      "Sentence 5: جو دونوں کو اپس میں جوڑ کر\n"
     ]
    }
   ],
   "source": [
    "# Set limit for how many sentences to print\n",
    "sentence_limit = 5\n",
    "\n",
    "# Rule-based SBD using POS tags (AUX as sentence boundary indicator)\n",
    "sentences = []\n",
    "current_sentence = []\n",
    "\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        current_sentence.append(word.text)\n",
    "        \n",
    "        # End the sentence if POS is 'AUX' (Auxiliary Verb)\n",
    "        if word.pos == 'AUX':\n",
    "            sentences.append(' '.join(current_sentence))\n",
    "            current_sentence = []\n",
    "    \n",
    "    if len(sentences) >= sentence_limit:\n",
    "        break\n",
    "\n",
    "# Append the last sentence if there is any remaining text\n",
    "if current_sentence and len(sentences) < sentence_limit:\n",
    "    sentences.append(' '.join(current_sentence))\n",
    "\n",
    "# Output only the limited number of sentences\n",
    "for idx, sentence in enumerate(sentences[:sentence_limit]):\n",
    "    print(f'Sentence {idx+1}: {sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences detected using AUX rule: 15018\n"
     ]
    }
   ],
   "source": [
    "# Rule-based SBD using POS tags (AUX as sentence boundary indicator)\n",
    "sentences = []\n",
    "current_sentence = []\n",
    "\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        current_sentence.append(word.text)\n",
    "        \n",
    "        # End the sentence if POS is 'AUX' (Auxiliary Verb)\n",
    "        if word.pos == 'AUX':\n",
    "            sentences.append(' '.join(current_sentence))\n",
    "            current_sentence = []\n",
    "\n",
    "# Append the last sentence if there is any remaining text\n",
    "if current_sentence:\n",
    "    sentences.append(' '.join(current_sentence))\n",
    "\n",
    "# Count the number of detected sentences\n",
    "detected_sentence_count = len(sentences)\n",
    "print(f'Number of sentences detected using AUX rule: {detected_sentence_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences detected using PRON/DET rule: 21050\n"
     ]
    }
   ],
   "source": [
    "# Rule-based SBD using POS tags (PRON and DET as sentence beginnings)\n",
    "sentences = []\n",
    "current_sentence = []\n",
    "\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        # If we encounter PRON or DET, start a new sentence\n",
    "        if word.pos == 'PRON' or word.pos == 'DET':\n",
    "            if current_sentence:  # If there's already content, save the previous sentence\n",
    "                sentences.append(' '.join(current_sentence))\n",
    "                current_sentence = []\n",
    "        \n",
    "        # Add the word to the current sentence\n",
    "        current_sentence.append(word.text)\n",
    "\n",
    "# Append the last sentence if there is any remaining text\n",
    "if current_sentence:\n",
    "    sentences.append(' '.join(current_sentence))\n",
    "\n",
    "# Count the number of detected sentences\n",
    "detected_sentence_count = len(sentences)\n",
    "print(f'Number of sentences detected using PRON/DET rule: {detected_sentence_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences detected using VERB + AUX rule: 10278\n"
     ]
    }
   ],
   "source": [
    "# Rule-based SBD using VERB followed by AUX as sentence end\n",
    "sentences = []\n",
    "current_sentence = []\n",
    "previous_word = None\n",
    "\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        current_sentence.append(word.text)\n",
    "\n",
    "        # Check if the current word is AUX and the previous word is VERB\n",
    "        if previous_word and previous_word.pos == 'VERB' and word.pos == 'AUX':\n",
    "            sentences.append(' '.join(current_sentence))\n",
    "            current_sentence = []  # Reset the current sentence\n",
    "        \n",
    "        # Update the previous word to the current one for the next iteration\n",
    "        previous_word = word\n",
    "\n",
    "# Append the last sentence if there is any remaining text\n",
    "if current_sentence:\n",
    "    sentences.append(' '.join(current_sentence))\n",
    "\n",
    "# Count the number of detected sentences\n",
    "detected_sentence_count = len(sentences)\n",
    "print(f'Number of sentences detected using VERB + AUX rule: {detected_sentence_count}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
